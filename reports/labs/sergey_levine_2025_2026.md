# Sergey Levine ç»„è¿‘æœŸé‡è¦å·¥ä½œè°ƒç ” (2025-2026)

> è°ƒç ”æ—¥æœŸ: 2026-02-15  
> è°ƒç ”äºº: Amy  
> æ¥æº: arXiv  

---

## ğŸ”¥ æœ€å€¼å¾—å…³æ³¨çš„æ–°ä½œ

### 1. Q-learning with Adjoint Matching (QAM)
- **æ ¸å¿ƒ**: è§£å†³è¿ç»­åŠ¨ä½œ RL ä¸­æ‰©æ•£/æµåŒ¹é…ç­–ç•¥çš„é«˜æ•ˆä¼˜åŒ–é—®é¢˜
- **åˆ›æ–°**: åˆ©ç”¨ critic çš„ä¸€é˜¶ä¿¡æ¯è¿›è¡Œç­–ç•¥ä¼˜åŒ–
- **é“¾æ¥**: [arXiv:2601.14234](https://arxiv.org/abs/2601.14234)

### 2. RoboReward: General-Purpose Vision-Language Reward Models
- **æ ¸å¿ƒ**: ç”¨ VLM ä½œä¸ºæœºå™¨äººä»»åŠ¡çš„é€šç”¨å¥–åŠ±æ¨¡å‹
- **æ„ä¹‰**: è§£å†³æ‰‹å·¥è®¾è®¡å¥–åŠ±å‡½æ•°çš„é—®é¢˜ï¼Œå®ç°çœŸå®æœºå™¨äººä»»åŠ¡çš„è‡ªåŠ¨å¥–åŠ±ç”Ÿæˆ
- **é“¾æ¥**: [arXiv:2601.00675](https://arxiv.org/abs/2601.00675)

### 3. SteerVLA: Steering VLA Models in Long-Tail Driving
- **æ ¸å¿ƒ**: è®© VLA æ¨¡å‹å¤„ç†é•¿å°¾é©¾é©¶åœºæ™¯
- **åˆ›æ–°**: ç»“åˆé«˜å±‚è¯­ä¹‰æ¨ç†ä¸åº•å±‚ååº”æ§åˆ¶
- **é“¾æ¥**: [arXiv:2602.08440](https://arxiv.org/abs/2602.08440)

---

## ğŸ¤– Agentic RL / VLA æ–¹å‘

### 4. Emergence of Human to Robot Transfer in VLA Models
- ä»äººç±»è§†é¢‘ä¸­å­¦ä¹ å¹¶è¿ç§»åˆ°æœºå™¨äººç­–ç•¥
- [arXiv:2512.22414](https://arxiv.org/abs/2512.22414)

### 5. Natural Language Actor-Critic: Scalable Off-Policy Learning
- åœ¨è¯­è¨€ç©ºé—´ä¸­è¿›è¡Œç¦»ç­–ç•¥å­¦ä¹ çš„ Actor-Critic æ–¹æ³•
- [arXiv:2512.04601](https://arxiv.org/abs/2512.04601)

### 6. Training-Time Action Conditioning for Real-Time Chunking
- é™ä½ VLA æ¨¡å‹æ¨ç†å»¶è¿Ÿçš„æ–¹æ³•
- [arXiv:2512.05964](https://arxiv.org/abs/2512.05964)

### 7. Beyond Sight: Multi-Sensory Robot Policies
- èåˆè§†è§‰ã€è§¦è§‰ã€éŸ³é¢‘çš„å¤šæ¨¡æ€æœºå™¨äººç­–ç•¥
- [arXiv:2501.04693](https://arxiv.org/abs/2501.04693)

---

## ğŸ§  Reasoning & Test-Time Compute

### 8. Zero-Overhead Introspection for Adaptive Test-Time Compute
- LLM è‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œé¢„æµ‹æˆåŠŸæ¦‚ç‡å’Œæ‰€éœ€è®¡ç®—é‡
- [arXiv:2512.01457](https://arxiv.org/abs/2512.01457)

### 9. Scaling Test-Time Compute Without Verification or RL is Suboptimal
- åˆ†æäº† test-time scaling çš„ä¸¤ç§æ–¹æ³•ï¼šè’¸é¦ vs RL + verification
- [arXiv:2502.12118](https://arxiv.org/abs/2502.12118)

---

## ğŸ“Š RL ç†è®ºä¸ç®—æ³•

### 10. SFT Memorizes, RL Generalizes â­
- **æ ¸å¿ƒå‘ç°**: SFT å¯¼è‡´è®°å¿†ï¼ŒRL å¸¦æ¥æ³›åŒ–
- **å®éªŒ**: åœ¨æ–‡æœ¬è§„åˆ™å˜ä½“å’Œè§†è§‰å˜ä½“ä¸Šå¯¹æ¯”ç ”ç©¶
- **ç½‘ç«™**: https://tianzhechu.com/SFTvsRL
- [arXiv:2501.17161](https://arxiv.org/abs/2501.17161)

### 11. Posterior Behavioral Cloning
- é¢„è®­ç»ƒ BC ç­–ç•¥ä»¥é«˜æ•ˆè¿›è¡Œ RL å¾®è°ƒ
- [arXiv:2512.16911](https://arxiv.org/abs/2512.16911)

### 12. Value-Based Deep RL Scales Predictably (ICML 2025)
- è¯æ˜ value-based ç¦»çº¿ RL æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯é¢„æµ‹æ‰©å±•æ€§
- [arXiv:2502.04327](https://arxiv.org/abs/2502.04327)

### 13. Flow Q-Learning (ICML 2025)
- åŸºäºæµåŒ¹é…çš„ç¦»çº¿ RL æ–¹æ³•
- [arXiv:2502.02538](https://arxiv.org/abs/2502.02538)

---

## ğŸ”§ ç³»ç»Ÿä¸è¯„ä¼°

### 14. PolaRiS: Scalable Real-to-Sim Evaluations
- çœŸå®åˆ°ä»¿çœŸçš„å¯æ‰©å±•è¯„ä¼°æ¡†æ¶
- **ç½‘ç«™**: https://polaris-evals.github.io/
- [arXiv:2512.16881](https://arxiv.org/abs/2512.16881)

### 15. Digi-Q: Learning Q-Functions for Device-Control Agents
- ICLR 2025ï¼Œç”¨äºè®¾å¤‡æ§åˆ¶æ™ºèƒ½ä½“çš„ Q å­¦ä¹ 
- [arXiv:2502.15760](https://arxiv.org/abs/2502.15760)

---

## ğŸ“ˆ è¶‹åŠ¿æ€»ç»“

1. **VLA + RL**: å¤§é‡å·¥ä½œèšç„¦ Vision-Language-Action æ¨¡å‹çš„ RL å¾®è°ƒ
2. **Test-Time Compute**: æ¨ç†æ—¶è®¡ç®—ä¼˜åŒ–æˆä¸ºçƒ­ç‚¹
3. **Real-World Robotics**: ä»ä»¿çœŸèµ°å‘çœŸå®æœºå™¨äººåº”ç”¨
4. **Reward Design**: è‡ªåŠ¨åŒ–å¥–åŠ±è®¾è®¡ï¼ˆVLM as reward modelï¼‰
5. **Theory**: å…³æ³¨ RL çš„æ³›åŒ– vs è®°å¿†é—®é¢˜

---

## ç›¸å…³æ ‡ç­¾

`#RL` `#VLA` `#Robotics` `#Test-Time-Compute` `#ICML2025` `#ICLR2025`
