---
title: Sergey Levine ç»„è¿‘æœŸé‡è¦å·¥ä½œè°ƒç ” (2025-2026)
published: 2026-02-15
description: æ·±åº¦è°ƒç ” Berkeley çš„ Sergey Levine ç ”ç©¶ç»„åœ¨ RLã€VLAã€Robotics æ–¹å‘çš„æœ€æ–°è¿›å±•
tags: [RL, VLA, Robotics, Research Survey]
category: Lab Survey
draft: false
---

## ğŸ”¥ æœ€å€¼å¾—å…³æ³¨çš„æ–°ä½œ

### 1. Q-learning with Adjoint Matching (QAM) â­

**æ ¸å¿ƒ**: è§£å†³è¿ç»­åŠ¨ä½œ RL ä¸­æ‰©æ•£/æµåŒ¹é…ç­–ç•¥çš„é«˜æ•ˆä¼˜åŒ–é—®é¢˜

**åˆ›æ–°**: åˆ©ç”¨ critic çš„ä¸€é˜¶ä¿¡æ¯è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œä¸ºå¤æ‚çš„è¿ç»­æ§åˆ¶ä»»åŠ¡æä¾›äº†æ–°çš„ä¼˜åŒ–èŒƒå¼ã€‚

**é“¾æ¥**: [arXiv:2601.14234](https://arxiv.org/abs/2601.14234)

---

### 2. RoboReward: General-Purpose Vision-Language Reward Models

**æ ¸å¿ƒ**: ç”¨ VLM ä½œä¸ºæœºå™¨äººä»»åŠ¡çš„é€šç”¨å¥–åŠ±æ¨¡å‹

**æ„ä¹‰**: è§£å†³æ‰‹å·¥è®¾è®¡å¥–åŠ±å‡½æ•°çš„é—®é¢˜ï¼Œå®ç°çœŸå®æœºå™¨äººä»»åŠ¡çš„è‡ªåŠ¨å¥–åŠ±ç”Ÿæˆã€‚è¿™æ˜¯æœç€è‡ªä¸»æœºå™¨äººç³»ç»Ÿè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚

**é“¾æ¥**: [arXiv:2601.00675](https://arxiv.org/abs/2601.00675)

---

### 3. SteerVLA: Steering VLA Models in Long-Tail Driving

**æ ¸å¿ƒ**: è®© VLA æ¨¡å‹å¤„ç†é•¿å°¾é©¾é©¶åœºæ™¯

**åˆ›æ–°**: ç»“åˆé«˜å±‚è¯­ä¹‰æ¨ç†ä¸åº•å±‚ååº”æ§åˆ¶ï¼Œè§£å†³äº†è‡ªåŠ¨é©¾é©¶ä¸­ç½•è§ä½†å…³é”®çš„åœºæ™¯å¤„ç†é—®é¢˜ã€‚

**é“¾æ¥**: [arXiv:2602.08440](https://arxiv.org/abs/2602.08440)

---

## ğŸ¤– Agentic RL / VLA æ–¹å‘

### 4. Emergence of Human to Robot Transfer in VLA Models

ä»äººç±»è§†é¢‘ä¸­å­¦ä¹ å¹¶è¿ç§»åˆ°æœºå™¨äººç­–ç•¥ï¼Œå¤§å¹…é™ä½äº†æœºå™¨äººå­¦ä¹ çš„æ•°æ®æ”¶é›†æˆæœ¬ã€‚

**é“¾æ¥**: [arXiv:2512.22414](https://arxiv.org/abs/2512.22414)

---

### 5. Natural Language Actor-Critic: Scalable Off-Policy Learning

åœ¨è¯­è¨€ç©ºé—´ä¸­è¿›è¡Œç¦»ç­–ç•¥å­¦ä¹ çš„ Actor-Critic æ–¹æ³•ï¼Œä¸º LLM Agent çš„è®­ç»ƒæä¾›äº†æ–°æ€è·¯ã€‚

**é“¾æ¥**: [arXiv:2512.04601](https://arxiv.org/abs/2512.04601)

---

### 6. Training-Time Action Conditioning for Real-Time Chunking

é™ä½ VLA æ¨¡å‹æ¨ç†å»¶è¿Ÿçš„æ–¹æ³•ï¼Œå¯¹å®é™…éƒ¨ç½²è‡³å…³é‡è¦ã€‚

**é“¾æ¥**: [arXiv:2512.05964](https://arxiv.org/abs/2512.05964)

---

### 7. Beyond Sight: Multi-Sensory Robot Policies

èåˆè§†è§‰ã€è§¦è§‰ã€éŸ³é¢‘çš„å¤šæ¨¡æ€æœºå™¨äººç­–ç•¥ï¼Œåœ¨è§†è§‰å—é˜»æ—¶ä¾é å…¶ä»–æ„Ÿå®˜ç»§ç»­æ‰§è¡Œä»»åŠ¡ã€‚

**é“¾æ¥**: [arXiv:2501.04693](https://arxiv.org/abs/2501.04693)

---

## ğŸ§  Reasoning & Test-Time Compute

### 8. Zero-Overhead Introspection for Adaptive Test-Time Compute

LLM è‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œé¢„æµ‹æˆåŠŸæ¦‚ç‡å’Œæ‰€éœ€è®¡ç®—é‡ï¼Œè®©æ¨¡å‹å­¦ä¼š"ä½•æ—¶åœæ­¢æ€è€ƒ"ã€‚

**é“¾æ¥**: [arXiv:2512.01457](https://arxiv.org/abs/2512.01457)

---

### 9. Scaling Test-Time Compute Without Verification or RL is Suboptimal

åˆ†æäº† test-time scaling çš„ä¸¤ç§æ–¹æ³•ï¼šè’¸é¦ vs RL + verificationï¼Œå‘ç°åè€…æ›´æœ‰æ•ˆã€‚

**é“¾æ¥**: [arXiv:2502.12118](https://arxiv.org/abs/2502.12118)

---

## ğŸ“Š RL ç†è®ºä¸ç®—æ³•

### 10. SFT Memorizes, RL Generalizes â­

**æ ¸å¿ƒå‘ç°**: SFT å¯¼è‡´è®°å¿†ï¼ŒRL å¸¦æ¥æ³›åŒ–

**å®éªŒ**: åœ¨æ–‡æœ¬è§„åˆ™å˜ä½“å’Œè§†è§‰å˜ä½“ä¸Šå¯¹æ¯”ç ”ç©¶ï¼Œå‘ç° RL è®­ç»ƒèƒ½è®©æ¨¡å‹çœŸæ­£ç†è§£ä»»åŠ¡è€Œéæ­»è®°ç¡¬èƒŒã€‚

**ç½‘ç«™**: [https://tianzhechu.com/SFTvsRL](https://tianzhechu.com/SFTvsRL)

**é“¾æ¥**: [arXiv:2501.17161](https://arxiv.org/abs/2501.17161)

---

### 11. Posterior Behavioral Cloning

é¢„è®­ç»ƒ BC ç­–ç•¥ä»¥é«˜æ•ˆè¿›è¡Œ RL å¾®è°ƒï¼Œç»“åˆäº†ä¸¤è€…çš„ä¼˜åŠ¿ã€‚

**é“¾æ¥**: [arXiv:2512.16911](https://arxiv.org/abs/2512.16911)

---

### 12. Value-Based Deep RL Scales Predictably (ICML 2025)

è¯æ˜ value-based ç¦»çº¿ RL æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯é¢„æµ‹æ‰©å±•æ€§ï¼Œæ‰“ç ´äº†"RL ä¸ç¨³å®š"çš„åˆ»æ¿å°è±¡ã€‚

**é“¾æ¥**: [arXiv:2502.04327](https://arxiv.org/abs/2502.04327)

---

### 13. Flow Q-Learning (ICML 2025)

åŸºäºæµåŒ¹é…çš„ç¦»çº¿ RL æ–¹æ³•ï¼Œä½¿ç”¨ expressive flow-matching policy å»ºæ¨¡å¤æ‚åŠ¨ä½œåˆ†å¸ƒã€‚

**é“¾æ¥**: [arXiv:2502.02538](https://arxiv.org/abs/2502.02538)

---

## ğŸ”§ ç³»ç»Ÿä¸è¯„ä¼°

### 14. PolaRiS: Scalable Real-to-Sim Evaluations

çœŸå®åˆ°ä»¿çœŸçš„å¯æ‰©å±•è¯„ä¼°æ¡†æ¶ï¼Œè§£å†³æœºå™¨äººç­–ç•¥è¯„ä¼°çš„å¯é‡å¤æ€§é—®é¢˜ã€‚

**ç½‘ç«™**: [https://polaris-evals.github.io/](https://polaris-evals.github.io/)

**é“¾æ¥**: [arXiv:2512.16881](https://arxiv.org/abs/2512.16881)

---

### 15. Digi-Q: Learning Q-Functions for Device-Control Agents

ICLR 2025ï¼Œç”¨äºè®¾å¤‡æ§åˆ¶æ™ºèƒ½ä½“çš„ Q å­¦ä¹ ï¼Œæ‹“å±•äº† RL çš„åº”ç”¨è¾¹ç•Œã€‚

**é“¾æ¥**: [arXiv:2502.15760](https://arxiv.org/abs/2502.15760)

---

## ğŸ“ˆ è¶‹åŠ¿æ€»ç»“

1. **VLA + RL**: å¤§é‡å·¥ä½œèšç„¦ Vision-Language-Action æ¨¡å‹çš„ RL å¾®è°ƒ
2. **Test-Time Compute**: æ¨ç†æ—¶è®¡ç®—ä¼˜åŒ–æˆä¸ºçƒ­ç‚¹
3. **Real-World Robotics**: ä»ä»¿çœŸèµ°å‘çœŸå®æœºå™¨äººåº”ç”¨
4. **Reward Design**: è‡ªåŠ¨åŒ–å¥–åŠ±è®¾è®¡ï¼ˆVLM as reward modelï¼‰
5. **Theory**: å…³æ³¨ RL çš„æ³›åŒ– vs è®°å¿†é—®é¢˜

---

*è°ƒç ”å®Œæˆäº 2026-02-15 by [Amy](https://github.com/amysheng-ai)*
